---
title: "725_Project"
author: "Becca Halford"
date: "2025-04-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(utils)
library(forecast)
library(tidyverse)
library(fable)
library(readxl)
library(ggplot2)
library(lubridate)
library(dplyr)
library(prophet)
library(rsample)
```

# Section 1

## Load Data

```{r uploading data from LCL}

# set to local environment
# DATA_FILE <- "C:/Users/Becca/Downloads/DSCI 725/Project/ehb_checkouts.xlsx"
DATA_FILE <- "ehb_checkouts.xlsx"
raw_data <- read_excel(DATA_FILE)

# display first few rows
raw_data 

```
# Section 2

## Preprocessing

Most of the preprocessing of this data set was completed in excel, since this data came from my job at the library.

```{r preprocessing}

# select only the columns you want
desired_cols <- c('Date',
                  'Day_of_Week_Numeric',
                  'Is_Closed',
                  'Days_Until_Holiday',
                  'New',
                  'Stacks',
                  'Total_Checkouts'
                  )
clean_data <- raw_data[(desired_cols)]

# SIDEKICK: TBD Consider "casting" days until holiday to <fct> (factor type), and is closed to <lgl> (logical type) ###Ryan, can you help me with this?

clean_data

```
### SIDEKICK: Data Dictionary

***Consider adding a data dictionary, e.g. a description of rows and columns, and what they signify.***

# Section 3

## Data Exploration

In this section, I will explore my data to get an idea of its shape and take note of anything that stands out.......

Plotting the daily checkouts in the boxplot, I can see a median of around 35-40 book checkouts per day. The bulk of checkouts fall between 15 to 55 checkouts per day. There are a few outliers, which show unusually high checkout days. These days had over 100 checkouts.

The histogram shows that most days have a lower number of checkouts. The most common range was 0-10 checkouts per day. There are fewer days with higher checkouts. There is a long tail to the right. Overall there is a moderate level of activity with most days falling in the range of 0-60 checkouts per day. As we have seen in the box plot, there are days where there are over 100 checkouts per day. This is rare, but possible.

```{r summary of data}

# summarize data
summary(clean_data)

```

```{r}
# Individual data points
ggplot(clean_data, aes(x = "", y = Total_Checkouts)) +
  geom_boxplot(fill = "purple", alpha = 0.6) +
  geom_jitter(width = 0.1, color = "black", alpha = 0.3) +
  labs(title = "Daily Checkouts", y = "Number of Checkouts", x = "") +
  theme_minimal()

```



```{r}
# histogram of total daily checkouts
hist(clean_data$Total_Checkouts, main="Daily Checkouts", 
     xlab="Number of Checkouts per Day", col="lightpink", xlim=c(0, 135))
```


```{r data visualizations}

# convert clean_data to dataframe for better feature engineering
clean_df <- data.frame(clean_data)

agg_data <- clean_df |>  # newer R pipe operator
  group_by(Day_of_Week_Numeric) |>
  summarise(Total_Checkouts = sum(Total_Checkouts, na.rm = TRUE)) 

agg_data

```

```{r}
# Remove the NA rows if they exist
agg_data_clean <- agg_data |>
  filter(!is.na(Day_of_Week_Numeric))

# Match Monday = 1, Sunday = 7
agg_data_clean <- agg_data_clean |>
  mutate(Day_of_Week_Label = factor(Day_of_Week_Numeric,
                                    levels = 1:7,
                                    labels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"),
                                    ordered = TRUE))

# Plot Daily Checkouts as a function of the day of week
ggplot(agg_data_clean, aes(x = Day_of_Week_Label, y = Total_Checkouts)) +
  geom_bar(stat = "identity", fill = "purple") +
  labs(title = "Total Book Checkouts by Day of the Week",
       x = "Day of the Week",
       y = "Total Checkouts") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),        # Center title
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```

It should be noted that there are so few checkouts on Sunday since the library is closed that day; the only checkouts on that day are from librarians themselves. It looks like Wednesdays are the most popular day for book checkouts - maybe this indicates that people are bored in the middle of the workweek? Weekdays are clearly much busier than weekends. 

```{r data viz holiday/checkouts}

# remove null rows
clean_df |>
  filter(is.na(Days_Until_Holiday) | is.na(Total_Checkouts))

# calculate and plot the percentage of checkouts on days leading up to a holiday
checkouts_percent <- clean_df |>
  filter(!is.na(Days_Until_Holiday), !is.na(Total_Checkouts)) |>
  group_by(Days_Until_Holiday) |>
  summarise(Total_Checkouts = sum(Total_Checkouts)) |>
  ungroup() |>
  mutate(Percent_Checkouts = Total_Checkouts / sum(Total_Checkouts) * 100)

# ggplot(checkouts_percent, aes(x = Days_Until_Holiday, y = Percent_Checkouts)) +
#   geom_bar(stat = "identity", fill = "skyblue") +
#   labs(title = "Percentage of Total Checkouts by Days Until Holiday",
#        x = "Days Until Holiday",
#        y = "Percentage of Total Checkouts") +
#   theme_minimal() +
#   theme(plot.title = element_text(hjust = 0.5))
# scale_x_reverse()

# Step 2: Create the bar plot
ggplot(checkouts_percent, aes(x = factor(Days_Until_Holiday), y = Percent_Checkouts)) +
  geom_bar(stat = "identity", fill = "purple") +
  geom_text(aes(label = paste0(round(Percent_Checkouts, 1), "%")), vjust = -0.5) +  # Add labels
  labs(title = "Percentage of Total Checkouts Within 0-5+ days of a Holiday",
       x = "Days Until Holiday",
       y = "Percentage of Total Checkouts") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
scale_x_reverse()
```
Unsurprisingly, most book checkouts are on days where there are more than 5 days until a holiday. This is expected since the data is heavily skewed towards non-holiday days. To get a better idea of what checkouts look like in the time period of interest (the days leading up to a holiday), I want to look at the distribution without the skewed rows. 

```{r data viz holiday/checkouts V2}

# calculate and plot the percentage of checkouts on days 0-5 leading up to a holiday

# Recalculate checkouts_percent properly with filtering FIRST
checkouts_percent <- clean_df |>
  filter(Days_Until_Holiday %in% 0:5, !is.na(Total_Checkouts)) |>
  mutate(Days_Until_Holiday = as.numeric(Days_Until_Holiday)) |>
  group_by(Days_Until_Holiday) |>
  summarise(Total_Checkouts = sum(Total_Checkouts)) |>
  ungroup() |>
  mutate(Percent_Checkouts = Total_Checkouts / sum(Total_Checkouts) * 100)

# Now plot it
ggplot(checkouts_percent, aes(x = Days_Until_Holiday, y = Percent_Checkouts)) +
  geom_bar(stat = "identity", fill = "#66c2a5") +
  geom_text(aes(label = sprintf("%.1f%%", Percent_Checkouts)),
            vjust = -0.3, size = 4) +
  scale_x_reverse(breaks = 0:5) +
  labs(
    title = "Checkouts in the Week Leading Up to a Holiday",
    x = "Days Before Holiday",
    y = "Percentage of Total Checkouts"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))


```

With the bar chart only showing up to 5 days before a holiday, we can see that 5 days before the holiday has the highest checkout rate of 24.5%. This could indicate that people tend to prepare for the library closure in advance. There is a fairly steady decline in the checkout rate (except for a small increase 3 days before a holiday) leading up to a holiday.

### SIDEKICK: Time Series Plotting

***It's a good idea to include a plot of checkouts over time -- here's an example.***

```{r time_series_visualization}
# Ensure data is properly ordered by date
clean_df <- clean_df[order(clean_df$Date), ]

# Create basic time series plot
ggplot(clean_df, aes(x = Date, y = Total_Checkouts)) +
  geom_line(color = "steelblue", linewidth = 0.7) +
  geom_point(color = "steelblue", alpha = 0.5, size = 1.5) +
  labs(
    title = "Daily Book Checkouts Over Time",
    subtitle = "Time Series of Library Checkout Activity",
    x = "Date",
    y = "Number of Checkouts"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) +
  # Add a smoothed trend line to help visualize the overall pattern
  geom_smooth(method = "loess", color = "darkred", se = FALSE, linetype = "dashed", span = 0.3)

# For a more detailed analysis with moving average
library(zoo)
clean_df$MA7 <- rollmean(clean_df$Total_Checkouts, k = 7, fill = NA)  # 7-day moving average

# Plot with moving average
ggplot(clean_df, aes(x = Date)) +
  geom_line(aes(y = Total_Checkouts), color = "steelblue", alpha = 0.7) +
  geom_line(aes(y = MA7), color = "darkred", linewidth = 1) +
  labs(
    title = "Daily Book Checkouts with 7-Day Moving Average",
    x = "Date",
    y = "Number of Checkouts"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

# Section 4

## ADF test 

First run ADF to test if the data is stationary

```{r adf-naive-yearly}
library(tseries)

# Ensure data is ordered by date
clean_df <- clean_df |> arrange(Date)

# Create Time Series with yearly seasonality
ts_data_yearly <- ts(
  clean_df$Total_Checkouts,
  frequency = 365,  # Yearly seasonality
  start = c(2022, yday(as.Date("2022-02-07")))
)

adf.test(ts_data_yearly)
```
```{r adf-naive-weekly}
# Create Time Series with weekly seasonality
ts_data_weekly <- ts(
  clean_df$Total_Checkouts,
  frequency = 7  # Weekly seasonality
)

adf.test(ts_data_weekly)
```
The p-value is < 0.01, which means we can reject the null hypothesis and conclude the yearly and weekly time series are both stationary.

## Modeling Checkouts with Linear Regression



## Forecasting using Time Series Methods


Now that I have an idea of what my data looks like, I need to train and test several different models.  My goal here is to train the following models: 

1. Naive Forecasting 
2. ARIMA Forecasting
3. Linear Regression


I also want to see how the models perform when they are trained on data that includes the skewed `Days_until_holiday` metric (>5) and when they are trained on data that excludes these rows.


### Naive Model


#### Naive Yearly Model

```{r naive_yearly}

# Split into training and test sets (last 7 days for testing)
train_ts_yearly <- head(ts_data_yearly, -7)  # all but last 7 days
test_ts_yearly <- tail(ts_data_yearly, 7)    # last 7 days

# Fit Naive Forecast Model on Training Set
naive_model_yearly <- naive(train_ts_yearly, h = length(test_ts_yearly))

# Get the 2025 data only (for plotting)
# First, identify the time points that are in 2025
dates_all <- seq(from = as.Date("2022-02-07"), 
                by = "day", 
                length.out = length(ts_data_yearly))
indices_2025 <- which(format(dates_all, "%Y") == "2025")

# Extract only last few (e.g., 30 days) observations from training data plus forecast
n_days_to_show <- 30
recent_train_indices <- tail(seq_along(train_ts_yearly), n_days_to_show)

# Create a focused plot
plot(naive_model_yearly, 
     main = "Naive Forecast (Yearly Frequency) - 2025 Only",
     xlab = "Date", 
     ylab = "Total Checkouts",
     xlim = c(tail(time(train_ts_yearly), 1) - 0.1, tail(time(test_ts_yearly), 1) + 0.1),
     ylim = c(0, max(c(tail(train_ts_yearly, n_days_to_show), test_ts_yearly, naive_model_yearly$mean)) * 1.2))

# Add a legend
legend("topright", 
       legend = c("Historical Data", "Forecast", "Actual Test Data"), 
       col = c("black", "blue", "red"), 
       lty = c(1, 1, NA), 
       pch = c(NA, NA, 19))

# Evaluate Forecast Accuracy
accuracy_metrics_yearly <- accuracy(naive_model_yearly, test_ts_yearly)
print("Naive Model (Yearly Frequency) Accuracy on Test Set:")
print(accuracy_metrics_yearly)
```

#### Naive Seasonal Weekly Model

```{r snaive_weekly}


# Split into training and test sets (last 7 days for testing)
train_ts_weekly <- head(ts_data_weekly, -7)  # all but last 7 days
test_ts_weekly <- tail(ts_data_weekly, 7)    # last 7 days

# Fit Seasonal Naive Forecast Model on Training Set
snaive_model_weekly <- snaive(train_ts_weekly, h = length(test_ts_weekly))

# For easier date handling with ggplot2, let's create a data frame
# Get the original dates
dates_all <- clean_df$Date[order(clean_df$Date)]

# Create a simpler data frame for plotting
plot_data <- data.frame(
  Date = c(
    # Last 28 days of training
    tail(dates_all[1:length(train_ts_weekly)], 28),
    # Forecast period (test period)
    tail(dates_all, 7)
  ),
  Value = c(
    # Last 28 days of actual values
    tail(as.numeric(train_ts_weekly), 28),
    # NA for the test period
    rep(NA, 7)
  ),
  Forecast = c(
    # NA for the historical period
    rep(NA, 28),
    # Forecast values
    as.numeric(snaive_model_weekly$mean)
  ),
  Actual = c(
    # NA for the historical period
    rep(NA, 28),
    # Actual test values
    as.numeric(test_ts_weekly)
  ),
  Lower = c(
    # NA for the historical period
    rep(NA, 28),
    # Lower prediction interval
    as.numeric(snaive_model_weekly$lower[, 1])
  ),
  Upper = c(
    # NA for the historical period
    rep(NA, 28),
    # Upper prediction interval
    as.numeric(snaive_model_weekly$upper[, 1])
  )
)

# Filter to only include 2025 dates
plot_data_2025 <- plot_data[format(plot_data$Date, "%Y") == "2025", ]

# Create improved plot with ggplot2
ggplot(plot_data_2025, aes(x = Date)) +
  # Historical data as black line
  geom_line(aes(y = Value), color = "black", na.rm = TRUE) +
  # Forecast as blue dashed line
  geom_line(aes(y = Forecast), color = "blue", linetype = "dashed", na.rm = TRUE) +
  # Actual test data as red points
  geom_point(aes(y = Actual), color = "red", size = 3, na.rm = TRUE) +
  # Prediction intervals as ribbon
  geom_ribbon(aes(ymin = Lower, ymax = Upper), fill = "blue", alpha = 0.2, na.rm = TRUE) +
  # Labels and theme
  labs(
    title = "Seasonal Naive Forecast (Weekly) vs Actual Checkouts - 2025 Only",
    subtitle = "Using same day from previous week as forecast",
    x = "Date",
    y = "Total Checkouts"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) +
  # Add legend
  annotate("segment", x = min(plot_data_2025$Date), xend = min(plot_data_2025$Date) + 5, 
           y = max(plot_data_2025$Upper, na.rm = TRUE) * 0.9, 
           yend = max(plot_data_2025$Upper, na.rm = TRUE) * 0.9, 
           color = "black", linewidth = 1) +
  annotate("text", x = min(plot_data_2025$Date) + 6, 
           y = max(plot_data_2025$Upper, na.rm = TRUE) * 0.9, 
           label = "Historical", hjust = 0) +
  
  annotate("segment", x = min(plot_data_2025$Date), xend = min(plot_data_2025$Date) + 5, 
           y = max(plot_data_2025$Upper, na.rm = TRUE) * 0.85, 
           yend = max(plot_data_2025$Upper, na.rm = TRUE) * 0.85, 
           color = "blue", linewidth = 1, linetype = "dashed") +
  annotate("text", x = min(plot_data_2025$Date) + 6, 
           y = max(plot_data_2025$Upper, na.rm = TRUE) * 0.85, 
           label = "Forecast", hjust = 0) +
  
  annotate("point", x = min(plot_data_2025$Date) + 2.5, 
           y = max(plot_data_2025$Upper, na.rm = TRUE) * 0.8, 
           color = "red", size = 3) +
  annotate("text", x = min(plot_data_2025$Date) + 6, 
           y = max(plot_data_2025$Upper, na.rm = TRUE) * 0.8, 
           label = "Actual", hjust = 0)

# Evaluate Forecast Accuracy
accuracy_metrics_weekly <- accuracy(snaive_model_weekly, test_ts_weekly)
print("Seasonal Naive Model (Weekly Frequency) Accuracy on Test Set:")
print(accuracy_metrics_weekly)
```


```{r compare_naive_yearly_weekly}
# Compare with regular naive model on same data
naive_model_weekly <- naive(train_ts_weekly, h = length(test_ts_weekly))
accuracy_metrics_naive_weekly <- accuracy(naive_model_weekly, test_ts_weekly)
print("Regular Naive Model (Weekly Frequency) Accuracy on Test Set:")
print(accuracy_metrics_naive_weekly)

# Create a comparison table of MASE values
model_comparison <- data.frame(
  Model = c("Naive (Weekly)", "Seasonal Naive (Weekly)"),
  MASE = c(
    accuracy_metrics_naive_weekly["Test set", "MASE"],
    accuracy_metrics_weekly["Test set", "MASE"]
  )
)
print("MASE Comparison:")
print(model_comparison)
```


### ARIMA

```{r}
# Step 1: Arrange data
clean_df <- clean_df |> arrange(Date)

# Step 2: Create time series object with weekly seasonality
ts_data <- ts(clean_df$Total_Checkouts, frequency = 7)

# Step 3: Split data (last 7 days for test set)
train_ts <- head(ts_data, -7)
test_ts <- tail(ts_data, 7)

# Step 4: Fit ARIMA model with improved parameters
# Use higher max.p, max.q, max.P, max.Q to allow for more complex models
# Set seasonal=TRUE to consider weekly seasonality
# Use stepwise=FALSE and approximation=FALSE for more thorough search
arima_model <- auto.arima(train_ts, 
                          seasonal = TRUE,      # Allow seasonal component
                          stepwise = FALSE,     # More thorough search
                          approximation = FALSE, # More accurate
                          trace = TRUE,         # Show model selection process
                          max.p = 5,            # Higher max for AR terms
                          max.q = 5,            # Higher max for MA terms
                          max.P = 5,            # Higher max for seasonal AR
                          max.Q = 5,            # Higher max for seasonal MA
                          max.d = 5,            # Allow up to 2 differences
                          max.D = 5,
                          allowdrift = TRUE)            # Allow 1 seasonal difference

# Print model summary
summary(arima_model)

# Step 5: Forecast the next 7 days
arima_forecast <- forecast(arima_model, h = 7)

# Step 6: Evaluate forecast accuracy
arima_accuracy <- accuracy(arima_forecast, test_ts)
print("ARIMA Model Accuracy:")
print(arima_accuracy)

# Step 7: Create a data frame for plotting 2025 data only
dates_all <- clean_df$Date[order(clean_df$Date)]

# Create a data frame for plotting
plot_data <- data.frame(
  Date = c(
    # Last 28 days of training
    tail(dates_all[1:length(train_ts)], 28),
    # Forecast period (test period)
    tail(dates_all, 7)
  ),
  Value = c(
    # Last 28 days of actual values
    tail(as.numeric(train_ts), 28),
    # NA for the test period
    rep(NA, 7)
  ),
  Forecast = c(
    # NA for the historical period
    rep(NA, 28),
    # Forecast values
    as.numeric(arima_forecast$mean)
  ),
  Actual = c(
    # NA for the historical period
    rep(NA, 28),
    # Actual test values
    as.numeric(test_ts)
  ),
  Lower = c(
    # NA for the historical period
    rep(NA, 28),
    # Lower prediction interval (80%)
    as.numeric(arima_forecast$lower[, 1])
  ),
  Upper = c(
    # NA for the historical period
    rep(NA, 28),
    # Upper prediction interval (80%)
    as.numeric(arima_forecast$upper[, 1])
  )
)

# Filter to only include 2025 dates
plot_data_2025 <- plot_data[format(plot_data$Date, "%Y") == "2025", ]

# Create improved plot with ggplot2
ggplot(plot_data_2025, aes(x = Date)) +
  # Historical data as black line
  geom_line(aes(y = Value), color = "black", na.rm = TRUE) +
  # Forecast as blue dashed line
  geom_line(aes(y = Forecast), color = "blue", linetype = "dashed", na.rm = TRUE) +
  # Actual test data as red points
  geom_point(aes(y = Actual), color = "red", size = 3, na.rm = TRUE) +
  # Prediction intervals as ribbon
  geom_ribbon(aes(ymin = Lower, ymax = Upper), fill = "blue", alpha = 0.2, na.rm = TRUE) +
  # Labels and theme
  labs(
    title = "Seasonal ARIMA Forecast vs Actual Checkouts - 2025 Only",
    subtitle = paste("Model:", capture.output(arima_model)[1]),
    x = "Date",
    y = "Total Checkouts"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) +
  # Add legend
  annotate("segment", x = min(plot_data_2025$Date), xend = min(plot_data_2025$Date) + 5, 
           y = max(plot_data_2025$Upper, na.rm = TRUE) * 0.9, 
           yend = max(plot_data_2025$Upper, na.rm = TRUE) * 0.9, 
           color = "black", linewidth = 1) +
  annotate("text", x = min(plot_data_2025$Date) + 6, 
           y = max(plot_data_2025$Upper, na.rm = TRUE) * 0.9, 
           label = "Historical", hjust = 0) +
  
  annotate("segment", x = min(plot_data_2025$Date), xend = min(plot_data_2025$Date) + 5, 
           y = max(plot_data_2025$Upper, na.rm = TRUE) * 0.85, 
           yend = max(plot_data_2025$Upper, na.rm = TRUE) * 0.85, 
           color = "blue", linewidth = 1, linetype = "dashed") +
  annotate("text", x = min(plot_data_2025$Date) + 6, 
           y = max(plot_data_2025$Upper, na.rm = TRUE) * 0.85, 
           label = "Forecast", hjust = 0) +
  
  annotate("point", x = min(plot_data_2025$Date) + 2.5, 
           y = max(plot_data_2025$Upper, na.rm = TRUE) * 0.8, 
           color = "red", size = 3) +
  annotate("text", x = min(plot_data_2025$Date) + 6, 
           y = max(plot_data_2025$Upper, na.rm = TRUE) * 0.8, 
           label = "Actual", hjust = 0)
```

```{r}
# Check for residual autocorrelation
checkresiduals(arima_model)

# Compare with seasonal naive model on same data
snaive_model <- snaive(train_ts, h = 7)
snaive_accuracy <- accuracy(snaive_model, test_ts)

# Create a comparison table of accuracy metrics
comparison_df <- data.frame(
  Model = c("Seasonal ARIMA", "Seasonal Naive"),
  RMSE = c(arima_accuracy["Test set", "RMSE"], 
           snaive_accuracy["Test set", "RMSE"]),
  MAE = c(arima_accuracy["Test set", "MAE"], 
          snaive_accuracy["Test set", "MAE"]),
  MASE = c(arima_accuracy["Test set", "MASE"], 
           snaive_accuracy["Test set", "MASE"])
)

print("Model Comparison:")
print(comparison_df)
```

***Sidekick: Excellent! The seasonal ARIMA is a good fit***

Here are my comments on the evaluation metrics for the above model:

### Exponential Smoothing

```{r}
# Step 1: Arrange data
clean_df <- clean_df |> arrange(Date)

# Step 2: Create time series object with weekly seasonality
ts_data <- ts(clean_df$Total_Checkouts, frequency = 7)

# Step 3: Split data (last 7 days for test set)
train_ts <- head(ts_data, -7)
test_ts <- tail(ts_data, 7)

# Step 4: Fit ETS (Error, Trend, Seasonal) model
# Let the function automatically select the best model type
ets_model <- ets(train_ts, model = "ZZZ")

# Print model summary
summary(ets_model)
```


```{r}
# Step 5: Forecast the next 7 days
ets_forecast <- forecast(ets_model, h = 7)

# Step 6: Evaluate forecast accuracy
ets_accuracy <- accuracy(ets_forecast, test_ts)
print("Exponential Smoothing Model Accuracy:")
print(ets_accuracy)
```

```{r}
# Step 7: Create a data frame for plotting 2025 data only
dates_all <- clean_df$Date[order(clean_df$Date)]

# Create a data frame for plotting
plot_data <- data.frame(
  Date = c(
    # Last 28 days of training
    tail(dates_all[1:length(train_ts)], 28),
    # Forecast period (test period)
    tail(dates_all, 7)
  ),
  Value = c(
    # Last 28 days of actual values
    tail(as.numeric(train_ts), 28),
    # NA for the test period
    rep(NA, 7)
  ),
  Forecast = c(
    # NA for the historical period
    rep(NA, 28),
    # Forecast values
    as.numeric(ets_forecast$mean)
  ),
  Actual = c(
    # NA for the historical period
    rep(NA, 28),
    # Actual test values
    as.numeric(test_ts)
  ),
  Lower = c(
    # NA for the historical period
    rep(NA, 28),
    # Lower prediction interval (80%)
    as.numeric(ets_forecast$lower[, 1])
  ),
  Upper = c(
    # NA for the historical period
    rep(NA, 28),
    # Upper prediction interval (80%)
    as.numeric(ets_forecast$upper[, 1])
  )
)

# Filter to only include 2025 dates
plot_data_2025 <- plot_data[format(plot_data$Date, "%Y") == "2025", ]

# Create improved plot with ggplot2
ggplot(plot_data_2025, aes(x = Date)) +
  # Historical data as black line
  geom_line(aes(y = Value), color = "black", na.rm = TRUE) +
  # Forecast as blue dashed line
  geom_line(aes(y = Forecast), color = "blue", linetype = "dashed", na.rm = TRUE) +
  # Actual test data as red points
  geom_point(aes(y = Actual), color = "red", size = 3, na.rm = TRUE) +
  # Prediction intervals as ribbon
  geom_ribbon(aes(ymin = Lower, ymax = Upper), fill = "blue", alpha = 0.2, na.rm = TRUE) +
  # Labels and theme
  labs(
    title = "Exponential Smoothing Forecast vs Actual Checkouts - 2025 Only",
    subtitle = paste("Model:", ets_model$method),
    x = "Date",
    y = "Total Checkouts"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) +
  # Add legend
  annotate("segment", x = min(plot_data_2025$Date), xend = min(plot_data_2025$Date) + 5, 
           y = max(plot_data_2025$Upper, na.rm = TRUE) * 0.9, 
           yend = max(plot_data_2025$Upper, na.rm = TRUE) * 0.9, 
           color = "black", linewidth = 1) +
  annotate("text", x = min(plot_data_2025$Date) + 6, 
           y = max(plot_data_2025$Upper, na.rm = TRUE) * 0.9, 
           label = "Historical", hjust = 0) +
  
  annotate("segment", x = min(plot_data_2025$Date), xend = min(plot_data_2025$Date) + 5, 
           y = max(plot_data_2025$Upper, na.rm = TRUE) * 0.85, 
           yend = max(plot_data_2025$Upper, na.rm = TRUE) * 0.85, 
           color = "blue", linewidth = 1, linetype = "dashed") +
  annotate("text", x = min(plot_data_2025$Date) + 6, 
           y = max(plot_data_2025$Upper, na.rm = TRUE) * 0.85, 
           label = "Forecast", hjust = 0) +
  
  annotate("point", x = min(plot_data_2025$Date) + 2.5, 
           y = max(plot_data_2025$Upper, na.rm = TRUE) * 0.8, 
           color = "red", size = 3) +
  annotate("text", x = min(plot_data_2025$Date) + 6, 
           y = max(plot_data_2025$Upper, na.rm = TRUE) * 0.8, 
           label = "Actual", hjust = 0)
```

## Model Comparison

```{r}
# Update comparison table with basic models only (excluding ARIMAX and Prophet)
final_comparison_df <- data.frame(
  Model = c("Naive", 
            "Seasonal Naive (Weekly)", 
            "ARIMA", 
            "Exponential Smoothing"),
  
  # RMSE values
  RMSE = c(accuracy_metrics_naive_weekly["Test set", "RMSE"],
           accuracy_metrics_weekly["Test set", "RMSE"],
           arima_accuracy["Test set", "RMSE"],
           ets_accuracy["Test set", "RMSE"]),
  
  # MAE values
  MAE = c(accuracy_metrics_naive_weekly["Test set", "MAE"],
          accuracy_metrics_weekly["Test set", "MAE"],
          arima_accuracy["Test set", "MAE"],
          ets_accuracy["Test set", "MAE"]),
  
  # MASE values - key metric for model comparison
  MASE = c(accuracy_metrics_naive_weekly["Test set", "MASE"],
           accuracy_metrics_weekly["Test set", "MASE"],
           arima_accuracy["Test set", "MASE"],
           ets_accuracy["Test set", "MASE"]),
  
  # Model descriptions
  Description = c(
    "Uses previous day's value as forecast",
    "Uses same day from previous week",
    paste0("ARIMA", capture.output(arima_model)[1]),
    paste0("ETS(", ets_model$components[1], ",", 
           ets_model$components[2], ",", 
           ets_model$components[3], ")")
  )
)

# Round numeric columns to 3 decimal places
final_comparison_df[, c("RMSE", "MAE", "MASE")] <- round(final_comparison_df[, c("RMSE", "MAE", "MASE")], 3)

# Find the best model (lowest MASE)
best_model_index <- which.min(final_comparison_df$MASE)
best_model_name <- final_comparison_df$Model[best_model_index]

# Create beautiful table with kable and kableExtra
library(knitr)
library(kableExtra)

# Create the enhanced table
final_comparison_table <- final_comparison_df %>%
  kable(caption = "Model Performance Comparison (Test Set)", 
        format = "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = FALSE,
                position = "center") %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(4, background = ifelse(final_comparison_df$MASE == min(final_comparison_df$MASE), 
                                    "#e6ffe6", "white")) %>%
  add_header_above(c(" " = 1, "Error Metrics" = 3, " " = 1)) %>%
  row_spec(best_model_index - 1, bold = TRUE, background = "#f2f2f2") %>%
  footnote(general = paste0("The best performing model is: ", best_model_name, 
                           " (lowest MASE = ", 
                           round(min(final_comparison_df$MASE), 3), ")"),
           symbol = c("MASE (Mean Absolute Scaled Error) is scale-independent and allows fair comparison between models.",
                      "Lower values for all metrics indicate better model performance."))

# Display the enhanced table
final_comparison_table

# Create a bar chart visualization of MASE values
library(ggplot2)

# Sort models by performance (using MASE)
final_comparison_df_sorted <- final_comparison_df[order(final_comparison_df$MASE), ]

# Add model type as a factor for coloring
final_comparison_df_sorted$Type <- factor(
  ifelse(grepl("Naive", final_comparison_df_sorted$Model), "Naive",
         ifelse(grepl("ARIMA", final_comparison_df_sorted$Model), "ARIMA",
                "Exponential Smoothing")),
  levels = c("Naive", "ARIMA", "Exponential Smoothing")
)

# Create beautiful bar chart of MASE values (lower is better)
ggplot(final_comparison_df_sorted, aes(x = reorder(Model, MASE), y = MASE, fill = Type)) +
  geom_bar(stat = "identity", width = 0.7) +
  geom_text(aes(label = sprintf("%.3f", MASE)), 
            hjust = -0.3, size = 3.5) +
  scale_fill_brewer(palette = "Set2") +
  coord_flip() +  # Horizontal bars for better readability with longer model names
  labs(
    title = "Model Comparison by MASE",
    subtitle = "Lower values indicate better performance",
    y = "MASE (Mean Absolute Scaled Error)",
    x = NULL,
    fill = "Model Type"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5),
    legend.position = "bottom"
  )
```

# Section 5

## Final Model Evaluation

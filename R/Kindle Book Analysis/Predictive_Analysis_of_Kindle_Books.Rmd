---
title: 'DSCI 726: Project 4 — Predictive Analysis for Book Recommendations'
author: "Becca Halford"
date: "2025-10-03"
output:
  word_document:
    toc: true
    toc_depth: '3'
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
    toc_depth: 3
    code_folding: hide
    theme: cosmo
  pdf_document:
    toc: true
    toc_depth: 3
---

# Executive Summary

This project predicts the star rating of new books by authors who have published at least three titles on Amazon. I used a random 80/20 split by book title to create the training and test sets. 

The modeling methods include a baseline model, a simple linear model, a full linear model, and a pruned decision tree. I evaluated model performance using RMSE, R^2, and the train-test performance gap. I also performed 5-fold cross-validation on the full linear model to assess its stability and generalizability.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# ---- Packages ----
library(tidyverse)
library(readxl)
library(car)         # VIF
library(corrplot)
library(knitr)
library(openxlsx)
library(caret)
library(rpart)
library(rpart.plot)
library(gridExtra)
library(patchwork)
library(viridis)     # colorblind-friendly palettes
library(e1071)       # skewness/kurtosis

# ---- Output folders ----
output_dir <- "predictive_analysis_output"
dir.create(output_dir, showWarnings = FALSE)
tables_dir <- file.path(output_dir, "tables"); dir.create(tables_dir, showWarnings = FALSE)
plots_dir  <- file.path(output_dir, "plots");  dir.create(plots_dir,  showWarnings = FALSE)

# ---- Helper functions ----
# calculate_rmse(actual, predicted) -> numeric: RMSE in star units
calculate_rmse <- function(actual, predicted) sqrt(mean((actual - predicted)^2))

# evaluate_model(model, train_data, test_data) -> list of preds + RMSE
evaluate_model <- function(model, train_data, test_data) {
  train_pred <- predict(model, train_data)
  test_pred  <- predict(model, test_data)
  list(
    train_pred = train_pred,
    test_pred  = test_pred,
    train_rmse = calculate_rmse(train_data$stars, train_pred),
    test_rmse  = calculate_rmse(test_data$stars,  test_pred)
  )
}

# save_plot(plot_obj, filename, width, height): convenient plot saver
save_plot <- function(plot_obj, filename, width = 8, height = 6) {
  print(plot_obj)
  ggsave(file.path(plots_dir, filename), plot = plot_obj, width = width, height = height)
}

# tabular VIF with status
check_vif <- function(model, model_name) {
  vif_values <- vif(model)
  vif_df <- data.frame(Variable = names(vif_values), VIF = round(vif_values, 2)) |>
    mutate(Status = case_when(
      VIF > 10 ~ "Remove - severe collinearity",
      VIF >  5 ~ "Monitor - moderate collinearity",
      TRUE     ~ "Keep"
    ))
  print(kable(vif_df, caption = paste("VIF Analysis —", model_name)))
  vif_df
}
```

# 1. Data Preparation

The column "save" was renamed to "savings" for clarity.
The column "size" was removed due to redundancy.
Duplicate rows were removed.
Converted applicable columns to integer.

## 1.1 Load & Clean

```{r load-and-clean}
kindle_data <- read_excel('cleaned_kindle_books.xlsx') |>
  select(-size) |>
  mutate(across(c(pages, author_num_books, customer_reviews), as.integer)) |>
  distinct(title, .keep_all = TRUE) |>
  rename(savings = save)

cat("Dataset:", nrow(kindle_data), "books ×", ncol(kindle_data), "variables\n")
glimpse(kindle_data)
```

## 1.2 Filter by author productivity (≤2 books removed)

Authors with 2 or less books were removed from the dataset to more accurately represent stars and avg_stars.

```{r author-filter}
original_n <- nrow(kindle_data)
author_counts <- kindle_data |> count(author, name = "n_books")
few_book_authors <- author_counts |> filter(n_books <= 2) |> pull(author)

kindle_data <- kindle_data |> filter(!author %in% few_book_authors)

cat("Removed:", original_n - nrow(kindle_data), "rows (", 
    round(100*(original_n - nrow(kindle_data))/original_n, 1), "%)\n", sep = "")
```

## 1.3 Train/Test Split

I split the data using a simple random sample by book title after completing the filtering process. The final split allocated 80% of the data to the training set (16,438 rows) and 20% to the test set (4,109 rows).

```{r split}
set.seed(123)
test_titles <- sample(kindle_data$title, size = floor(0.2 * nrow(kindle_data)))
train_set <- kindle_data |> filter(!title %in% test_titles)
test_set  <- kindle_data |> filter(title %in% test_titles)

cat("Train/Test sizes:", nrow(train_set), "/", nrow(test_set), "\n")
```

# 2. Feature Engineering

A log transformation was performed due to highly skewed variables.

## 2.1 Transformations

```{r transforms}
cols_to_log <- c("price", "customer_reviews", "pages", "savings")

train_set <- train_set |>
  mutate(across(all_of(cols_to_log), ~log(.x + 1), .names = "log_{.col}"))
test_set  <- test_set  |>
  mutate(across(all_of(cols_to_log), ~log(.x + 1), .names = "log_{.col}"))

cat("Log-transformed:", paste(cols_to_log, collapse = ", "), "\n")
```

Log transformation was unsuccessful on 'author_num_books', so binning was applied.

## 2.2 Bin `author_num_books` (quartiles)

```{r bin-author-books}
quartiles <- quantile(train_set$author_num_books, probs = c(0, .25, .5, .75, 1), na.rm = TRUE)

train_set <- train_set |>
  mutate(author_books_bin = cut(author_num_books, breaks = quartiles,
                                labels = c("Few_Q1","Some_Q2","Many_Q3","Prolific_Q4"),
                                include.lowest = TRUE))
test_set <- test_set |>
  mutate(author_books_bin = cut(author_num_books, breaks = quartiles,
                                labels = c("Few_Q1","Some_Q2","Many_Q3","Prolific_Q4"),
                                include.lowest = TRUE))
table(train_set$author_books_bin)
```

## 2.3 Scale numeric features (train params)

Scaling was performed to standardize numeric features.

```{r scaling}
scale_cols <- c(paste0("log_", cols_to_log), "discount_pct", "avg_stars")

scaling_params <- train_set |>
  summarise(across(all_of(scale_cols), list(mean = ~mean(.x, na.rm = TRUE),
                                            sd   = ~sd(.x,   na.rm = TRUE))))

train_set_scaled <- train_set; test_set_scaled <- test_set
for (col in scale_cols) {
  m <- scaling_params[[paste0(col, "_mean")]]; s <- scaling_params[[paste0(col, "_sd")]]
  train_set_scaled[[col]] <- (train_set_scaled[[col]] - m) / s
  test_set_scaled[[col]]  <- (test_set_scaled[[col]]  - m) / s
}

train_set_scaled <- train_set_scaled |> mutate(author_books_bin = as.factor(author_books_bin))
test_set_scaled  <- test_set_scaled  |> mutate(author_books_bin = factor(author_books_bin,
                                                                         levels = levels(train_set_scaled$author_books_bin)))
# one-hot for binned variable
dummies <- dummyVars(~ author_books_bin, data = train_set_scaled)
train_cat <- as.data.frame(predict(dummies, newdata = train_set_scaled))
test_cat  <- as.data.frame(predict(dummies, newdata = test_set_scaled))

numeric_features <- c("log_price","log_customer_reviews","log_pages","log_savings","avg_stars","discount_pct","stars")

train_model <- cbind(train_set_scaled |> select(all_of(numeric_features)), train_cat)
test_model  <- cbind(test_set_scaled  |> select(all_of(numeric_features)), test_cat)

cat("Model matrices:", dim(train_model)[1], "×", dim(train_model)[2], "(train);\n",
    dim(test_model)[1], "×", dim(test_model)[2], "(test)\n")
```

# 3. Baselines (for context)

I include two simple baselines to set expectations:
Null Mean: predict the training mean rating.
Author‑only OLS: predict with `avg_stars` only.

```{r baselines}
null_pred <- rep(mean(train_model$stars), nrow(test_model))
null_rmse <- calculate_rmse(test_model$stars, null_pred)

author_only <- lm(stars ~ avg_stars, data = train_model)
author_only_res <- evaluate_model(author_only, train_model, test_model)
author_rmse <- author_only_res$test_rmse

cat("Baseline RMSE (Null Mean):", round(null_rmse, 4), "\n")
cat("Baseline RMSE (Author-only):", round(author_rmse, 4), "\n")
```

# 4. Correlation & Collinearity Checks

For linear regression I check VIF and explicitly remove a redundant predictor to avoid multicollinearity (`discount_pct` is dropped in favor of `log_savings`).  
For tree-based models I include use the pairwise correlation technique to flag redundancy.

```{r cor-matrix}
numeric_predictors <- c("log_price","log_customer_reviews","log_pages","log_savings","avg_stars","discount_pct")
cor_matrix <- cor(train_model |> select(all_of(numeric_predictors)), use = "pairwise.complete.obs")

par(oma = c(4, 0, 0, 0))
corrplot(cor_matrix, method = "color", type = "upper",
         tl.col = "black", tl.cex = 0.8, addCoef.col = "black",
         number.cex = 0.7, diag = FALSE)
mtext("Correlation Matrix — Multicollinearity Check", side = 1, line = .25, outer = TRUE, cex = 1)
```

# 5. Models

## 5.1 Simple OLS (baseline comparator; not useful)

I tried a limited OLS (log price, log reviews, log pages) purely as a baseline for comparison. It had low R² and no predictive value.

```{r simple-ols}
simple_model <- lm(stars ~ log_price + log_customer_reviews + log_pages, data = train_model)
simple_res   <- evaluate_model(simple_model, train_model, test_model)
simple_r2    <- summary(simple_model)$r.squared
cat("Simple OLS — Train/Test RMSE:", round(simple_res$train_rmse, 4), "/", round(simple_res$test_rmse, 4),
    "; R² =", round(simple_r2, 4), "\n")
vif_simple <- check_vif(simple_model, "Simple OLS")
write_csv(vif_simple, file.path(tables_dir, "vif_simple.csv"))
```

## 5.2 Full OLS

I excluded `discount_pct` (and keep `log_savings`) in the linear model to avoid multicollinearity. These two variables are strongly related and dropping one helps stabilize coefficient estimates.

```{r full-ols}
full_model <- lm(stars ~ log_price + log_customer_reviews + log_pages +
                         log_savings + avg_stars +
                         author_books_bin.Some_Q2 + author_books_bin.Many_Q3 + author_books_bin.Prolific_Q4,
                 data = train_model)

summary(full_model)

full_res   <- evaluate_model(full_model, train_model, test_model)
full_r2    <- summary(full_model)$r.squared
vif_full   <- check_vif(full_model, "Full OLS")
write_csv(vif_full, file.path(tables_dir, "vif_full.csv"))

cat("Full OLS — Train/Test RMSE:", round(full_res$train_rmse, 4), "/", round(full_res$test_rmse, 4),
    "; R² =", round(full_r2, 4), "\n")
```

## 5.3 Decision Tree (with cp-based pruning)

I still allow `discount_pct` in the tree since trees are less sensitive to multicollinearity.

```{r tree}
tree_model <- rpart(stars ~ log_price + log_customer_reviews + log_pages +
                            log_savings + avg_stars + discount_pct +
                            author_books_bin.Some_Q2 + author_books_bin.Many_Q3 + author_books_bin.Prolific_Q4,
                    data = train_model,
                    control = rpart.control(maxdepth = 5, minsplit = 100, cp = 0.01))

printcp(tree_model)
best_cp <- tree_model$cptable[which.min(tree_model$cptable[,"xerror"]), "CP"]
tree_model <- prune(tree_model, cp = best_cp)

tree_res <- evaluate_model(tree_model, train_model, test_model)
cat("Decision Tree (pruned) — Train/Test RMSE:",
    round(tree_res$train_rmse, 4), "/", round(tree_res$test_rmse, 4), "\n")
```

## 5.4 5-fold Cross-Validation (Full OLS)

Cross-validation shows stability across multiple subsets of data. 

```{r cv}
perform_cv <- function(formula, data, k = 5) {
  set.seed(123)
  n <- nrow(data); fold_size <- floor(n / k); shuffled <- sample(seq_len(n)); cv_errors <- numeric(k)
  for (i in 1:k) {
    idx <- if (i < k) shuffled[((i-1)*fold_size+1):(i*fold_size)] else shuffled[((i-1)*fold_size+1):n]
    m   <- lm(formula, data = data[-idx, ])
    pr  <- predict(m, newdata = data[idx, ])
    cv_errors[i] <- calculate_rmse(data$stars[idx], pr)
  }
  cv_errors
}

full_formula <- formula(stars ~ log_price + log_customer_reviews + log_pages +
                                 log_savings + avg_stars +
                                 author_books_bin.Some_Q2 + author_books_bin.Many_Q3 + author_books_bin.Prolific_Q4)

cv_errors <- perform_cv(full_formula, train_model, k = 5)
cv_mean_rmse <- mean(cv_errors); cv_sd_rmse <- sd(cv_errors)

cat("5-fold CV RMSEs:", paste(round(cv_errors, 4), collapse = ", "), "\n",
    "Mean (SD):", round(cv_mean_rmse, 4), "(", round(cv_sd_rmse, 4), ")\n")
```

# 6. Evaluation & Comparison

```{r compare}
simple_test_rmse <- simple_res$test_rmse
full_test_rmse   <- full_res$test_rmse
tree_test_rmse   <- tree_res$test_rmse

model_results <- tibble::tibble(
  Model       = c("Null Mean","Author-only","Simple OLS","Full OLS","Decision Tree (pruned)"),
  Predictors  = c(0, 1, 3, 8, 9),
  Train_RMSE  = c(calculate_rmse(train_model$stars, rep(mean(train_model$stars), nrow(train_model))),
                  author_only_res$train_rmse, simple_res$train_rmse, full_res$train_rmse, tree_res$train_rmse),
  Test_RMSE   = c(null_rmse, author_rmse, simple_test_rmse, full_test_rmse, tree_test_rmse),
  R_squared   = c(NA,
                  summary(author_only)$r.squared,
                  summary(simple_model)$r.squared,
                  summary(full_model)$r.squared,
                  cor(tree_res$test_pred, test_model$stars)^2),
  CV_RMSE     = c(NA, NA, NA, cv_mean_rmse, NA),
  Overfit_Gap = Test_RMSE - Train_RMSE
) |>
  mutate(across(where(is.numeric), ~round(.x, 4)))

kable(model_results, caption = "Model Performance Comparison")

write_csv(model_results, file.path(tables_dir, "model_comparison.csv"))

best_idx <- which.min(model_results$Test_RMSE)
cat("Best by Test RMSE:", model_results$Model[best_idx], "with RMSE =", model_results$Test_RMSE[best_idx], "\n")
```

## 6.1 Diagnostics (Full OLS)

```{r diagnostics, fig.height=8, fig.width=10}
create_diagnostic_plots <- function(actual, predicted, model_name) {
  residuals <- actual - predicted
  par(mfrow = c(2,2))
  plot(actual, predicted, main = paste(model_name,": Predicted vs Actual"),
       xlab = "Actual Stars", ylab = "Predicted Stars", pch = 19, col = rgb(0,0,0,.3)); abline(0,1,col="red",lwd=2)
  plot(predicted, residuals, main = "Residuals vs Fitted", xlab = "Fitted", ylab = "Residuals",
       pch = 19, col = rgb(0,0,0,.3)); abline(h=0,col="red",lwd=2,lty=2)
  qqnorm(residuals, main = "Normal Q-Q Plot"); qqline(residuals, col = "red", lwd = 2)
  hist(residuals, breaks = 30, main = "Residuals", xlab = "Residual", col = "lightblue", probability = TRUE)
  curve(dnorm(x, mean = mean(residuals), sd = sd(residuals)), add = TRUE, col = "red", lwd = 2)
  par(mfrow = c(1,1))
}

create_diagnostic_plots(test_model$stars, predict(full_model, test_model), "Full OLS")
```

# 7. Brief Findings & Limitations (in plain language)

- The author‑level `avg_stars` variable seems to explain most of the variation. That fits the idea that readers follow authors they already like.
- Book‑level variables play a smaller role here, and discounting doesn’t seem to move the rating itself (which is about perceived quality).
- Ratings are pretty tightly bunched around 4.0–4.6, so there is a natural ceiling that makes low RMSE hard to achieve.
- I did notice mild funneling in residuals. If I were taking this further, I would report robust standard errors for the regression.
- The analysis is aimed at authors with at least three books in the data; predicting ratings for brand‑new authors would likely need a different setup.

# Appendix A — Supplemental EDA (kept minimal for rubric compliance)

These visuals justify transformations and give a quick sense of feature–target relationships.

## A1. Post-Transformation Distributions

```{r appendix-dists, fig.height=8, fig.width=10}
transformed_features <- train_model |> select(starts_with("log_"), avg_stars, discount_pct)
n_total <- nrow(transformed_features)
plot_sample <- transformed_features |> slice_sample(n = min(5000, n_total))
trans_long <- plot_sample |> pivot_longer(everything(), names_to="feature", values_to="value")
p_dist <- ggplot(trans_long, aes(x=value)) + geom_histogram(bins=30, fill="steelblue", alpha=.7) +
  facet_wrap(~feature, scales="free") + theme_minimal() +
  labs(title = "Feature Distributions After Transformation", subtitle = "Sampled for visualization")
save_plot(p_dist, "feature_distributions.png", width = 10, height = 8)
```

## A2. Feature–Target Scatter with Linear Trend (sampled)

```{r appendix-bivariate, fig.height=10, fig.width=12}
n_train <- nrow(train_model)
train_sample <- train_model |> slice_sample(n = min(5000, n_train))
feats <- c("log_price","log_customer_reviews","log_pages","log_savings","avg_stars","discount_pct")
make_scatter <- function(df, x, y="stars") {
  ggplot(df, aes_string(x=x, y=y)) + geom_point(alpha=.3, size=.5) +
    geom_smooth(method="lm", se=TRUE, color="red", size=1) +
    theme_minimal() + labs(title=paste(x,"vs",y), subtitle="Linear trend on sample")
}
plots <- lapply(feats, function(f) make_scatter(train_sample, f))
do.call(grid.arrange, c(plots, ncol=3))
```

# 8. Save Key Outputs

```{r save-outputs}
write_csv(vif_simple, file.path(tables_dir, "vif_simple.csv"))
write_csv(vif_full,   file.path(tables_dir, "vif_full.csv"))
write_csv(model_results, file.path(tables_dir, "model_comparison.csv"))
writeLines(capture.output(sessionInfo()), file.path(output_dir, "session_info.txt"))
cat("All results saved to:", output_dir, "\n")
```
``` {r bounds}

# Numeric variables
numeric_vars <- c("log_price","log_customer_reviews","log_pages","log_savings","avg_stars")

numeric_bounds <- data.frame(
  variable = numeric_vars,
  min = sapply(numeric_vars, function(x) min(train_model[[x]], na.rm = TRUE)),
  max = sapply(numeric_vars, function(x) max(train_model[[x]], na.rm = TRUE))
)

# Binary variables
binary_vars <- c("author_books_bin.Some_Q2","author_books_bin.Many_Q3","author_books_bin.Prolific_Q4")

binary_bounds <- data.frame(
  variable = binary_vars,
  min = rep(0, length(binary_vars)),
  max = rep(1, length(binary_vars))
)

# Combine numeric and binary bounds
decision_bounds <- rbind(numeric_bounds, binary_bounds)

# Reset row names
rownames(decision_bounds) <- NULL

# Display the clean table
print(decision_bounds)


```